{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baovin/ML_homework/blob/main/Inclass_Lab02_problem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class environment:\n",
        "  def __init__(self, grid_height, grid_width):\n",
        "    \"\"\"\n",
        "    The map is initialized with height and width are varible of your choice\n",
        "    start: List of location where you step in, you get to the corresponding location in list 'end'\n",
        "           For example:\n",
        "              if you step in location start[3] then you get to new location end[3] then obtain the reward value of reward[3]\n",
        "    reward: List of reward value where you move from a location in 'start' list to the corresponding location in 'end' list\n",
        "    \"\"\"\n",
        "    self.height = grid_height\n",
        "    self.width = grid_width\n",
        "    self.start = []\n",
        "    self.end = []\n",
        "    self.reward = []\n",
        "    self.map = np.array([i for i in range(grid_height * grid_width)])\n",
        "    self.action_space = [0,1,2,3]\n",
        "\n",
        "  def get_Map(self):\n",
        "    print(self.map.reshape([self.width, self.height]))\n",
        "\n",
        "  def get_NumState(self):\n",
        "    return self.height * self.width\n",
        "\n",
        "  def map_Designate(self, start_cell, end_cell, reward):\n",
        "    self.start.append(start_cell)\n",
        "    self.end.append(end_cell)\n",
        "    self.reward.append(reward)\n",
        "\n",
        "  def get_Observation(self, location, action):\n",
        "    # Default reward = 0\n",
        "    reward = 0\n",
        "    new_location = 0\n",
        "    # Action: UP: 0, DOWN: 1, LEFT: 2, RIGHT: 3\n",
        "    # Actions that get the agent out of the map, result in no change at all\n",
        "    if action == 0: #UP\n",
        "      if location - self.width < 0:\n",
        "        new_location = location\n",
        "      else:\n",
        "        new_location = location - self.width\n",
        "\n",
        "    elif action == 1: #DOWN\n",
        "      if location + self.width > self.height * self.width - 1:\n",
        "        new_location = location\n",
        "      else:\n",
        "        new_location = location + self.width\n",
        "\n",
        "    elif action == 2: #LEFT\n",
        "      if location % self.width == 0:\n",
        "        new_location = location\n",
        "      else:\n",
        "        new_location = location - 1\n",
        "\n",
        "    elif action == 3: #RIGHT\n",
        "      if (location + 1) % self.width == 0:\n",
        "        new_location = location\n",
        "      else:\n",
        "        new_location = location + 1\n",
        "\n",
        "    # If the agent is at special locations, immediately moves to corresponding destinations, gain reward\n",
        "    if new_location in self.start:\n",
        "      idx = self.start.index(new_location)\n",
        "      new_location = self.end[idx]\n",
        "      reward = self.reward[idx]\n",
        "\n",
        "    return new_location, self.action_space, reward\n",
        "\n",
        "\n",
        "class MAB_agent:\n",
        "  def __init__(self, envir, init_location):\n",
        "    # Trace the reward\n",
        "    self.reward_trace = []\n",
        "    # initialize the first location\n",
        "    self.location_now = init_location\n",
        "    # TODO: implement other features to the agent so it can perform MAB algorithm\n",
        "    self.lastAction = None\n",
        "    self.lastState = None\n",
        "    self.value_table = {}    # format: {state : {action : [value, count]}}\n",
        "    for state in range(envir.width*envir.height):\n",
        "      self.value_table[state] = {}\n",
        "      for action in envir.action_space:\n",
        "        self.value_table[state][action] = [0,0]\n",
        "\n",
        "  def get_TotalReward(self):\n",
        "    return np.sum(self.reward_trace)\n",
        "\n",
        "  # Running in Simulator\n",
        "  def getAction(self, observation):\n",
        "    self.location_now, action_space, pre_reward = observation\n",
        "    # NOTICE: the first observation is (init_location, [0,1,2,3], None)\n",
        "    # You should process the 'None'\n",
        "    if pre_reward is None:\n",
        "      action = np.random.choice(action_space)\n",
        "    else:\n",
        "      self.reward_trace.append(pre_reward)\n",
        "      # Updating with incremental average over reward samples\n",
        "      value = self.value_table[self.lastState][self.lastAction][0]\n",
        "      count = self.value_table[self.lastState][self.lastAction][1]\n",
        "\n",
        "      count += 1\n",
        "      value += (1/count) * (pre_reward - value)\n",
        "\n",
        "      self.value_table[self.lastState][self.lastAction][0] = value\n",
        "      self.value_table[self.lastState][self.lastAction][1] = count\n",
        "\n",
        "      # # get action\n",
        "      # state_dict = self.value_table[self.location_now]\n",
        "      # max = max([])\n",
        "      # action, max_value = max(state_dict.items(), key=lambda x: x[1][0])\n",
        "\n",
        "      # get action\n",
        "      state_dict = self.value_table[self.location_now]\n",
        "      # action, max_value = max(state_dict.items(), key=lambda x: x[1][0])\n",
        "      list_value = [f[0] for f in state_dict.values()]\n",
        "      max_value = max(list_value)\n",
        "      # find all actions that have maximum value\n",
        "      max_actions = [index for index, value in enumerate(list_value) if value == max_value]\n",
        "      if len(max_actions) == 1:\n",
        "        action = max_actions[0]\n",
        "      else:\n",
        "        # find among maximum actions which has already done\n",
        "        freq_actions = [state_dict[a][1] for a in max_actions]\n",
        "        max_freq_actions = [index for index, value in enumerate(freq_actions) if value == max(freq_actions)]\n",
        "        if len(max_freq_actions) == 1:\n",
        "          index = max_freq_actions[0]\n",
        "        else:\n",
        "          # there are multiple actions that have same value, same frequent, use random\n",
        "          index = np.random.choice(max_freq_actions)\n",
        "        action = max_actions[index]\n",
        "\n",
        "\n",
        "    self.lastState = self.location_now\n",
        "    self.lastAction = action\n",
        "    # Assert valid action\n",
        "    assert action in action_space, \"INVALID action taken\"\n",
        "    return action\n",
        "\n",
        "\n",
        "class MABe_agent(MAB_agent):\n",
        "  def __init__(self, envir, init_location, epsilon):\n",
        "    super(MABe_agent, self).__init__(envir, init_location)\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "\n",
        "  # Override\n",
        "  def getAction(self, observation):\n",
        "    self.location_now, action_space, pre_reward = observation\n",
        "    # NOTICE: the first observation is (init_location, [0,1,2,3], None)\n",
        "    # You should process the 'None'\n",
        "\n",
        "    toss = np.random.rand()\n",
        "    if pre_reward is None or toss < self.epsilon:\n",
        "      action = np.random.choice(action_space)\n",
        "    else:\n",
        "      self.reward_trace.append(pre_reward)\n",
        "      # Updating with incremental average over reward samples\n",
        "      value = self.value_table[self.lastState][self.lastAction][0]\n",
        "      count = self.value_table[self.lastState][self.lastAction][1]\n",
        "\n",
        "      count += 1\n",
        "      value += (1/count) * (pre_reward - value)\n",
        "\n",
        "      self.value_table[self.lastState][self.lastAction][0] = value\n",
        "      self.value_table[self.lastState][self.lastAction][1] = count\n",
        "\n",
        "      # get action\n",
        "      state_dict = self.value_table[self.location_now]\n",
        "      # action, max_value = max(state_dict.items(), key=lambda x: x[1][0])\n",
        "      list_value = [f[0] for f in state_dict.values()]\n",
        "      max_value = max(list_value)\n",
        "      # find all actions that have maximum value\n",
        "      max_actions = [index for index, value in enumerate(list_value) if value == max_value]\n",
        "      if len(max_actions) == 1:\n",
        "        action = max_actions[0]\n",
        "      else:\n",
        "        # find among maximum actions which has already done\n",
        "        freq_actions = [state_dict[a][1] for a in max_actions]\n",
        "        min_freq_actions = [index for index, value in enumerate(freq_actions) if value == min(freq_actions)]\n",
        "        if len(min_freq_actions) == 1:\n",
        "          index = min_freq_actions[0]\n",
        "        else:\n",
        "          # there are multiple actions that have same value, same frequent, use random\n",
        "          index = np.random.choice(min_freq_actions)\n",
        "        action = max_actions[index]\n",
        "\n",
        "\n",
        "\n",
        "    self.lastState = self.location_now\n",
        "    self.lastAction = action\n",
        "    # Assert valid action\n",
        "    assert action in action_space, \"INVALID action taken\"\n",
        "    return action"
      ],
      "metadata": {
        "id": "bAl-IKaQoc-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzdIZxnSetq6"
      },
      "source": [
        "# Create environment\n",
        "Envir = environment(8,8)\n",
        "Envir.map_Designate(17,56,-15)\n",
        "Envir.map_Designate(18,56,-15)\n",
        "Envir.map_Designate(19,56,-15)\n",
        "Envir.map_Designate(21,56,-15)\n",
        "Envir.map_Designate(25,56,-15)\n",
        "Envir.map_Designate(33,56,-15)\n",
        "Envir.map_Designate(41,56,-15)\n",
        "Envir.map_Designate(42,56,-15)\n",
        "Envir.map_Designate(43,56,-15)\n",
        "Envir.map_Designate(46,56,-15)\n",
        "Envir.map_Designate(47,56,-15)\n",
        "Envir.map_Designate(47,56,-15)\n",
        "Envir.map_Designate(15,56,+15)\n",
        "Envir.map_Designate(1,10,+5)\n",
        "Envir.map_Designate(26,56,+20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Envir.get_Map()"
      ],
      "metadata": {
        "id": "J94woyICr3NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3biF3d_3NKp"
      },
      "source": [
        "###Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib_g2lsuhER8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-psM4c__dvfy"
      },
      "source": [
        "# Implement your agent\n",
        "class Q_agent(MAB_agent):\n",
        "  def __init__(self, envir, init_location, learning_rate, discount_factor, epsilon):\n",
        "    super(Q_agent, self).__init__(envir, init_location)\n",
        "    self.Q_table = {}   # format: {state : {action : [value, count]}}\n",
        "    for state in range(envir.width*envir.height):\n",
        "      self.Q_table[state] = {}\n",
        "      for action in envir.action_space:\n",
        "        self.Q_table[state][action] = [0,0]\n",
        "    self.alpha = learning_rate          # learning rate\n",
        "    self.gamma = discount_factor        # discount factor\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  # Overide method\n",
        "  def getAction(self, observation):\n",
        "    self.location_now, action_space, pre_reward = observation\n",
        "    # NOTICE: the first observation is (NONE, [0,1,2,3], None)\n",
        "    # You should process the 'None' value\n",
        "    if pre_reward is None:\n",
        "      # randomly select among action space\n",
        "      ########## YOUR CODE HERE ############\n",
        "    else:\n",
        "      # get action\n",
        "      ########## YOUR CODE HERE ############\n",
        "\n",
        "\n",
        "\n",
        "      # update Q-table\n",
        "      ########## YOUR CODE HERE ############\n",
        "\n",
        "\n",
        "\n",
        "    self.lastState = self.location_now\n",
        "    self.lastAction = action\n",
        "    # Assert valid action\n",
        "    assert action in action_space, \"INVALID action taken\"\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usN0muJGewT7"
      },
      "source": [
        "init_location=0\n",
        "dummy_q_agent = Q_agent(envir=Envir, init_location=init_location, learning_rate=0.8, discount_factor=0.8, epsilon=0.3)\n",
        "\n",
        "num_iter = 10000\n",
        "\n",
        "log_freq = 1000\n",
        "Data_plot1 = []\n",
        "\n",
        "for i in range(num_iter):\n",
        "  if i == 0:\n",
        "    env_observation = (init_location, Envir.action_space, None)\n",
        "  else:\n",
        "    env_observation = Envir.get_Observation(location=dummy_q_agent.location_now, action=chosen_action)\n",
        "\n",
        "  chosen_action = dummy_q_agent.getAction(observation=env_observation)\n",
        "\n",
        "  if (i + 1) % log_freq == 0:\n",
        "    aver = np.mean(dummy_q_agent.reward_trace)\n",
        "    Data_plot1.append(aver)\n",
        "    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummy_q_agent.get_TotalReward()) + '\\t Average: ' + str(aver))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip2Wifu-q35F"
      },
      "source": [
        "Y_Cord = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
        "X_Cord = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "\n",
        "# Map = freq_list.reshape([Envir.width, Envir.height])\n",
        "Map = np.zeros([Envir.width, Envir.height])\n",
        "for i in range(Envir.height):\n",
        "  for j in range(Envir.width):\n",
        "    state = i * Envir.width + j\n",
        "    if state in dummy_q_agent.Q_table.keys():\n",
        "      list_freq = [f[1] for f in dummy_q_agent.Q_table[state].values()]\n",
        "      Map[i][j] = sum(list_freq)\n",
        "    else:\n",
        "      Map[i][j] = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "im = ax.imshow(Map, cmap=\"summer\")\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(X_Cord)))\n",
        "ax.set_yticks(np.arange(len(Y_Cord)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(X_Cord)\n",
        "ax.set_yticklabels(Y_Cord)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(Y_Cord)):\n",
        "    for j in range(len(X_Cord)):\n",
        "        text = ax.text(j, i, Map[i, j],\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"Number of actions taken, \" + str(num_iter) + \" time-steps\", fontdict={'size':16})\n",
        "# ax.set_xlabel(\"Alpha = \" + str(learning_rate) + \" Gamma = \" + str(discount_factor))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Cord = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
        "X_Cord = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "# agent = dummyAgent_e\n",
        "\n",
        "# Map = freq_list.reshape([Envir.width, Envir.height])\n",
        "Map = np.zeros([Envir.width, Envir.height])\n",
        "for i in range(Envir.height):\n",
        "  for j in range(Envir.width):\n",
        "    state = i * Envir.width + j\n",
        "    if state in dummy_q_agent.Q_table.keys():\n",
        "      list_value = [round(f[0],1) for f in dummy_q_agent.Q_table[state].values()]\n",
        "      Map[i][j] = round(sum(list_value),1)\n",
        "    else:\n",
        "      Map[i][j] = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "im = ax.imshow(Map, cmap=\"summer\")\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(X_Cord)))\n",
        "ax.set_yticks(np.arange(len(Y_Cord)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(X_Cord)\n",
        "ax.set_yticklabels(Y_Cord)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(Y_Cord)):\n",
        "    for j in range(len(X_Cord)):\n",
        "        text = ax.text(j, i, Map[i, j],\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"State value after \" + str(num_iter) + \" time-steps\", fontdict={'size':16})\n",
        "# ax.set_xlabel(\"Alpha = \" + str(learning_rate) + \" Gamma = \" + str(discount_factor))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0TMvJfefq0mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Cord = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
        "X_Cord = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "\n",
        "\n",
        "def decode_action(action):\n",
        "  if action == 0: #UP\n",
        "    return \"\\u2191\"\n",
        "  if action == 1: #DOWN\n",
        "    return \"\\u2193\"\n",
        "  if action == 2: #LEFT\n",
        "    return \"\\u2190\"\n",
        "  return \"\\u2192\" #RIGHT\n",
        "\n",
        "Map = np.zeros([Envir.width, Envir.height])\n",
        "Map_labels = [['' for _ in range(Envir.width)] for _ in range(Envir.height)]\n",
        "for i in range(Envir.height):\n",
        "  for j in range(Envir.width):\n",
        "    state = i * Envir.width + j\n",
        "    if state in dummy_q_agent.Q_table.keys():\n",
        "      list_value = [f[0] for f in dummy_q_agent.Q_table[state].values()]\n",
        "      max_value = max(list_value)\n",
        "      actions = [index for index, value in enumerate(list_value) if value == max_value]\n",
        "      if len(actions) > 1:\n",
        "        Map_labels[i][j] = ''\n",
        "        Map[i][j] = 1\n",
        "      else:\n",
        "        Map_labels[i][j] = decode_action(actions[0])\n",
        "        Map[i][j] = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "im = ax.imshow(Map,cmap='summer')\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(X_Cord)))\n",
        "ax.set_yticks(np.arange(len(Y_Cord)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(X_Cord)\n",
        "ax.set_yticklabels(Y_Cord)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(Y_Cord)):\n",
        "    for j in range(len(X_Cord)):\n",
        "        text = ax.text(j, i, Map_labels[i][j],\n",
        "                       ha=\"center\", va=\"center\", color=\"k\",fontsize=20)\n",
        "\n",
        "ax.set_title(\"Directions after \" + str(num_iter) + \" time-steps\", fontdict={'size':16})\n",
        "# ax.set_xlabel(\"Alpha = \" + str(learning_rate) + \" Gamma = \" + str(discount_factor))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R1qdkt9wXDEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxeMZG08xcvg"
      },
      "source": [
        "Envir.get_Map()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQAQdEcW3HV6"
      },
      "source": [
        "###Expected SARSA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zZJOguz3G_q"
      },
      "source": [
        "# Implement your agent\n",
        "class eSARSA_agent(MAB_agent):\n",
        "  def __init__(self, envir, init_location, learning_rate, discount_factor, epsilon):\n",
        "    super(eSARSA_agent, self).__init__(envir, init_location)\n",
        "    # TODO: initialize your Q table\n",
        "    self.Q_table = {}   # format: {state : {action : [value, count]}}\n",
        "    for state in range(envir.width*envir.height):\n",
        "      self.Q_table[state] = {}\n",
        "      for action in envir.action_space:\n",
        "        self.Q_table[state][action] = [0,0]\n",
        "    self.alpha = learning_rate          # learning rate\n",
        "    self.gamma = discount_factor        # discount factor\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  # Overide method\n",
        "   def getAction(self, observation):\n",
        "    self.location_now, action_space, pre_reward = observation\n",
        "    # NOTICE: the first observation is (NONE, [0,1,2,3], None)\n",
        "    # You should process the 'None' value\n",
        "    if pre_reward is None:\n",
        "      # randomly select among action space\n",
        "      ########## YOUR CODE HERE ############\n",
        "    else:\n",
        "      # get action\n",
        "      ########## YOUR CODE HERE ############\n",
        "\n",
        "\n",
        "\n",
        "      # update Q-table\n",
        "      ########## YOUR CODE HERE ############\n",
        "\n",
        "    self.lastState = self.location_now\n",
        "    self.lastAction = action\n",
        "    # Assert valid action\n",
        "    assert action in action_space, \"INVALID action taken\"\n",
        "    return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_location=0\n",
        "dummy_esarsa_agent = eSARSA_agent(envir=Envir, init_location=init_location, learning_rate=0.8, discount_factor=0.8, epsilon=0.3)\n",
        "\n",
        "num_iter = 10000\n",
        "\n",
        "log_freq = 1000\n",
        "Data_plot1 = []\n",
        "\n",
        "for i in range(num_iter):\n",
        "  if i == 0:\n",
        "    env_observation = (init_location, Envir.action_space, None)\n",
        "  else:\n",
        "    env_observation = Envir.get_Observation(location=dummy_esarsa_agent.location_now, action=chosen_action)\n",
        "\n",
        "  chosen_action = dummy_esarsa_agent.getAction(observation=env_observation)\n",
        "\n",
        "  if (i + 1) % log_freq == 0:\n",
        "    aver = np.mean(dummy_esarsa_agent.reward_trace)\n",
        "    Data_plot1.append(aver)\n",
        "    print('iter: ' + str(i + 1) + '\\t Total reward: ' + str(dummy_esarsa_agent.get_TotalReward()) + '\\t Average: ' + str(aver))"
      ],
      "metadata": {
        "id": "h3AK3G_Wqv2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Cord = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
        "X_Cord = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "# agent = dummyAgent_e\n",
        "\n",
        "# Map = freq_list.reshape([Envir.width, Envir.height])\n",
        "Map = np.zeros([Envir.width, Envir.height])\n",
        "for i in range(Envir.height):\n",
        "  for j in range(Envir.width):\n",
        "    state = i * Envir.width + j\n",
        "    if state in dummy_esarsa_agent.Q_table.keys():\n",
        "      list_value = [round(f[0],1) for f in dummy_esarsa_agent.Q_table[state].values()]\n",
        "      Map[i][j] = round(sum(list_value),1)\n",
        "    else:\n",
        "      Map[i][j] = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "im = ax.imshow(Map, cmap=\"summer\")\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(X_Cord)))\n",
        "ax.set_yticks(np.arange(len(Y_Cord)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(X_Cord)\n",
        "ax.set_yticklabels(Y_Cord)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(Y_Cord)):\n",
        "    for j in range(len(X_Cord)):\n",
        "        text = ax.text(j, i, Map[i, j],\n",
        "                       ha=\"center\", va=\"center\", color=\"k\")\n",
        "\n",
        "ax.set_title(\"State value after \" + str(num_iter) + \" time-steps\", fontdict={'size':16})\n",
        "# ax.set_xlabel(\"Alpha = \" + str(learning_rate) + \" Gamma = \" + str(discount_factor))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dvbnvdq6bTfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Cord = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
        "X_Cord = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n",
        "\n",
        "\n",
        "def decode_action(action):\n",
        "  if action == 0: #UP\n",
        "    return \"\\u2191\"\n",
        "  if action == 1: #DOWN\n",
        "    return \"\\u2193\"\n",
        "  if action == 2: #LEFT\n",
        "    return \"\\u2190\"\n",
        "  return \"\\u2192\" #RIGHT\n",
        "\n",
        "Map = np.zeros([Envir.width, Envir.height])\n",
        "Map_labels = [['' for _ in range(Envir.width)] for _ in range(Envir.height)]\n",
        "for i in range(Envir.height):\n",
        "  for j in range(Envir.width):\n",
        "    state = i * Envir.width + j\n",
        "    if state in dummy_esarsa_agent.Q_table.keys():\n",
        "      list_value = [f[0] for f in dummy_esarsa_agent.Q_table[state].values()]\n",
        "      max_value = max(list_value)\n",
        "      actions = [index for index, value in enumerate(list_value) if value == max_value]\n",
        "      if len(actions) > 1:\n",
        "        Map_labels[i][j] = ''\n",
        "        Map[i][j] = 1\n",
        "      else:\n",
        "        Map_labels[i][j] = decode_action(actions[0])\n",
        "        Map[i][j] = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "im = ax.imshow(Map,cmap='summer')\n",
        "\n",
        "# We want to show all ticks...\n",
        "ax.set_xticks(np.arange(len(X_Cord)))\n",
        "ax.set_yticks(np.arange(len(Y_Cord)))\n",
        "# ... and label them with the respective list entries\n",
        "ax.set_xticklabels(X_Cord)\n",
        "ax.set_yticklabels(Y_Cord)\n",
        "\n",
        "# Rotate the tick labels and set their alignment.\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Loop over data dimensions and create text annotations.\n",
        "for i in range(len(Y_Cord)):\n",
        "    for j in range(len(X_Cord)):\n",
        "        text = ax.text(j, i, Map_labels[i][j],\n",
        "                       ha=\"center\", va=\"center\", color=\"k\",fontsize=20)\n",
        "\n",
        "ax.set_title(\"Directions after \" + str(num_iter) + \" time-steps\", fontdict={'size':16})\n",
        "# ax.set_xlabel(\"Alpha = \" + str(learning_rate) + \" Gamma = \" + str(discount_factor))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ENTZewFb0uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYKVOUBwcAUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}